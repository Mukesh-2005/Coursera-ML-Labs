# ğŸ“Š Regularization in Linear Regression â€“ Evaluation Metrics Lab

This lab investigates how regularization techniquesâ€”**Ridge (L2)** and **Lasso (L1)**â€”affect linear regression performance under two conditions:
1. A simple 1-D dataset with and without outliers
2. A synthetic high-dimensional dataset with known informative features

The core focus is on **evaluation metrics**â€”how different models perform and generalize based on quantitative indicators like RÂ², MAE, MSE, and RMSE.

---

## ğŸ¯ Goal

To understand and compare model robustness, generalization, and feature selection capabilities using **evaluation metrics** as the primary lens.

---

## ğŸ“ Lab Structure

### 1ï¸âƒ£ Simple Regression with Outliers

- Generated synthetic data: \( y = 4 + 3X + \text{noise} \)
- Injected outliers at high \( X \) values
- Trained:
  - `LinearRegression`
  - `Ridge(alpha=1)`
  - `Lasso(alpha=0.2)`
- Compared fitted lines on clean vs. noisy data

### 2ï¸âƒ£ High-Dimensional Regression (100 Features)

- Used `make_regression(n_samples=100, n_features=100, n_informative=10, noise=10, coef=True)`
- Trained:
  - `LinearRegression`
  - `Ridge(alpha=1)`
  - `Lasso(alpha=0.1)`
- Compared predictions, coefficients, and residuals
- Used Lasso for feature selection and retrained models on reduced feature set

---

## ğŸ“Š Evaluation Metrics Used

| Metric | Description |
|--------|-------------|
| **RÂ² Score** | Measures proportion of variance explained |
| **Explained Variance** | Similar to RÂ², but less sensitive to bias |
| **Mean Absolute Error (MAE)** | Average absolute difference between predictions and actual values |
| **Mean Squared Error (MSE)** | Penalizes larger errors more heavily |
| **Root Mean Squared Error (RMSE)** | Square root of MSE for interpretability |

### ğŸ” Metric Comparison (Before Feature Selection)

| Model         | RÂ²     | MAE     | MSE     | RMSE    |
|---------------|--------|---------|---------|---------|
| Linear        | â‰ˆ 0.401 | â‰ˆ 77.75 | â‰ˆ 9855  | â‰ˆ 99.27 |
| Ridge         | â‰ˆ 0.408 | â‰ˆ 76.96 | â‰ˆ 9744  | â‰ˆ 98.71 |
| Lasso         | â‰ˆ 0.982 | â‰ˆ 13.89 | â‰ˆ 304.6 | â‰ˆ 17.45 |

### âœ… After Feature Selection (Lasso-selected features)

- All models achieved RÂ² â‰ˆ 0.98
- MAE and RMSE dropped significantly for Linear and Ridge
- Performance matched Lasso with fewer features

---

## ğŸ“ˆ Visual Diagnostics

- **Regression line plots** (with and without outliers)
- **Coefficient comparison**: True vs. predicted
- **Residual plots**: Ideal âˆ’ predicted
- **Feature selection thresholding** using Lasso coefficients

---

## ğŸ§  Key Takeaways

### ğŸ”¹ Outlier Sensitivity
- LinearRegression is highly sensitive to extreme values.
- Lasso showed better robustness due to L1 penalty.

### ğŸ”¹ Feature Selection
- Lasso effectively identified ~9 of 10 informative features.
- Retraining Linear/Ridge on selected features improved performance dramatically.

### ğŸ”¹ Metric-Driven Insights
- Evaluation metrics revealed model weaknesses and strengths more clearly than visual inspection alone.
- RMSE and MAE were especially useful for quantifying prediction error.

---
>Built by Mukesh â€” part of My Evaluation Metrics series.
ğŸ“… Last updated: November 2025

